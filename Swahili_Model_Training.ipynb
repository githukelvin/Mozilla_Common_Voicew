{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ar3FfvQ6GOS"
      },
      "source": [
        "Welcome to  nemo swahili model   train  code run  the  following  cells in the order  indicated e.g  \n",
        "#first step to  run\n",
        " above  will  be in each  cell\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below cell it will prompt you to connect to google  drive  click connect and select  the  account and allow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIjJVZSupoxu",
        "outputId": "0eb9f365-dcc3-4973-e250-aebb9f4d6edf"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After   connect  the "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!tar -xzvf \"/content/drive/Mozilla/eval0.tar.gz\" -C \"/content/drive/MyDrive/Mozilla/clips/\"     #[run this cell to extract tar.gz files]\n",
        "!tar -xzvf \"/content/drive/Mozilla/test0.tar.gz\" -C \"/content/drive/MyDrive/Mozilla/clips/\"     #[run this cell to extract tar.gz files]\n",
        "!tar -xzvf \"/content/drive/Mozilla/train0.tar.gz\" -C \"/content/drive/MyDrive/Mozilla/clips/\"     #[run this cell to extract tar.gz files]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJ6Z1BRQo5tu",
        "outputId": "1757d538-065f-45bf-a079-1c1f8ffc9ad8"
      },
      "outputs": [],
      "source": [
        "#second  step  run this  step  after\n",
        "\n",
        "!pip install --upgrade pip\n",
        "!pip install wget text-unidecode\n",
        "!python -m pip install git+https://github.com/NVIDIA/NeMo.git\n",
        "!pip install nemo_toolkit['all']\n",
        "!apt-get update && apt-get install -y libsndfile1 ffmpeg sox   libsox-fmt-mp3\n",
        "!pip install Cython\n",
        "!sudo apt-get update\n",
        "!sudo apt-get upgrade\n",
        "!sudo apt-get install lame\n",
        "!pip install sox lameenc jsonlines\n",
        "!pip install ffmpeg-python wget tensorflow\n",
        "!pip install text-unidecode pydub\n",
        "!pip install matplotlib>=3.3.2\n",
        "# Install NeMo\n",
        "BRANCH = 'main'\n",
        "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]\n",
        "!pip install tensorflow[and-cuda]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above cell  is of the utmost  importance it need to be ran second after cell 0 since in install all required libraries  nemo requires and other files  require eg ffmpeg "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below 4 cells are optional cells to  run if  you  don't have error with graphics on an issues are to be ran  if there is an issues  with your graphics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHiIjpx7_7vR",
        "outputId": "bd6a535e-7f96-41d3-ce92-86ff187579bd"
      },
      "outputs": [],
      "source": [
        "# third  step run this  as  the third  step\n",
        "# Important  this cell on running  it will require  you to press  the  output  cell and hit enter to continue\n",
        "\n",
        "# install nvidia & other dependencies\n",
        "!add-apt-repository ppa:graphics-drivers/ppa\n",
        "!apt update\n",
        "!apt install nvidia-384 nvidia-384-dev\n",
        "!apt-get install g++ freeglut3-dev build-essential libx11-dev libxmu-dev libxi-dev libglu1-mesa libglu1-mesa-dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVjacMkDk_5S"
      },
      "outputs": [],
      "source": [
        "#step  five\n",
        "\n",
        "# installing cuda\n",
        "\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin\n",
        "!sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/12.3.0/local_installers/cuda-repo-ubuntu2204-12-3-local_12.3.0-545.23.06-1_amd64.deb\n",
        "!sudo dpkg -i cuda-repo-ubuntu2204-12-3-local_12.3.0-545.23.06-1_amd64.deb\n",
        "!sudo cp /var/cuda-repo-ubuntu2204-12-3-local/cuda-*-keyring.gpg /usr/share/keyrings/\n",
        "!sudo apt-get update\n",
        "!sudo apt-get -y install cuda-toolkit-12-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tQwa2C2lAuW"
      },
      "outputs": [],
      "source": [
        "#step  six\n",
        "# # driver installer\n",
        "!sudo apt-get install -y cuda-drivers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMxVp1oxlD0G"
      },
      "outputs": [],
      "source": [
        "# step  seven\n",
        "# # nvidia toolkit cuda\n",
        "!apt-get install nvidia-cuda-toolkit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "above 4 cells  respectively are to be ran if you find  error in graphics  error so if there is no error skip the cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEEKJzFJExNf",
        "outputId": "7d969255-88e5-4b2c-889a-29e1b23390ba"
      },
      "outputs": [],
      "source": [
        "# step eight  to  show  if you are connected to the gpu\n",
        "import tensorflow as tf\n",
        "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXY8RoKAectC"
      },
      "outputs": [],
      "source": [
        "#fourth step  run this as 4th  step\n",
        "# show our  gpu\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above  2 cells is important to see if you have instantiated gpu  in colab or machine has gpu support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpH6qSTYpRNA"
      },
      "source": [
        "Below is code to convert .tsv to json file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# if you  didn't  have any issue and want  to continue  training  or transcribe the  files  skip  the following cells since  have prepared  the dataset skip till where  you find start  here  to train then transcribe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3 cells below is  to convert tsv and csv  files  respectively and directing them to  where their  files are located"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtzUrNSkqdz_"
      },
      "outputs": [],
      "source": [
        "!python drive/MyDrive/Mozilla/tsv_to_json.py \\\n",
        "  --tsv=drive/MyDrive/Mozilla/train.tsv \\\n",
        "  --folder=/content/drive/MyDrive/Mozilla/clips/train \\\n",
        "  --sampling_count=-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmn9CG32xlX_"
      },
      "outputs": [],
      "source": [
        "!python drive/MyDrive/Mozilla/csv_to_json.py \\\n",
        "  --csv=drive/MyDrive/Mozilla/eval.csv \\\n",
        "  --folder=/content/drive/MyDrive/Mozilla/clips/eval \\\n",
        "  --sampling_count=-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pqMrhokx13i"
      },
      "outputs": [],
      "source": [
        "!python drive/MyDrive/Mozilla/csv_to_json.py \\\n",
        "  --csv=drive/MyDrive/Mozilla/test.csv \\\n",
        "  --folder=/content/drive/MyDrive/Mozilla/clips/test \\\n",
        "  --sampling_count=-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below  3 cells  are to resample our files  from .mp3 to .wav with sample rate of 16000hz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-WJgtO8wY72"
      },
      "outputs": [],
      "source": [
        "!mkdir drive/MyDrive/Mozilla/train_data/\n",
        "!python drive/MyDrive/Mozilla/decode_resample.py \\\n",
        "  --manifest=drive/MyDrive/Mozilla/train.json \\\n",
        "  --destination_folder=drive/MyDrive/Mozilla/train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pf9HHXhj0guG"
      },
      "outputs": [],
      "source": [
        "!mkdir drive/MyDrive/Mozilla/eval_data/\n",
        "!python drive/MyDrive/Mozilla/csv_resample.py \\\n",
        "  --manifest=drive/MyDrive/hackathon/eval.json \\\n",
        "  --destination_folder=/content/drive/MyDrive/Mozilla/eval_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAq4LiXl0myO"
      },
      "outputs": [],
      "source": [
        "!mkdir drive/MyDrive/Mozilla/test_data/\n",
        "!python drive/MyDrive/Mozilla/csv_resample.py \\\n",
        "  --manifest=drive/MyDrive/Mozilla/test.json \\\n",
        "  --destination_folder=drive/MyDrive/Mozilla/test_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This  cell below  in simple terms we clean our audio files of unnecessary  character  eg special characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kla3wVyAwUgq"
      },
      "outputs": [],
      "source": [
        "# prepare_dataset_kiswahili.py\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from nemo.collections.asr.parts.utils.manifest_utils import read_manifest, write_manifest\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def write_processed_manifest(data, original_path):\n",
        "    original_manifest_name = os.path.basename(original_path)\n",
        "    new_manifest_name = original_manifest_name.replace(\".json\", \"_processed.json\")\n",
        "\n",
        "    manifest_dir = os.path.split(original_path)[0]\n",
        "    filepath = os.path.join(manifest_dir, new_manifest_name)\n",
        "    write_manifest(filepath, data)\n",
        "    print(f\"Finished writing manifest: {filepath}\")\n",
        "    return filepath\n",
        "\n",
        "\n",
        "# calculate the character set\n",
        "def get_charset(manifest_data):\n",
        "    charset = defaultdict(int)\n",
        "    for row in tqdm(manifest_data, desc=\"Computing character set\"):\n",
        "        text = row['text']\n",
        "        for character in text:\n",
        "            charset[character] += 1\n",
        "    return charset\n",
        "\n",
        "\n",
        "# Preprocessing steps\n",
        "\n",
        "def remove_special_characters(data):\n",
        "    chars_to_ignore_regex = \"[\\.\\,\\?\\:\\-!;()«»…\\]\\[/\\*–‽+&_\\\\½√>€™$•¼}{~—=“\\\"”″‟„]\"\n",
        "    apostrophes_regex = \"[’'‘`ʽ']\"\n",
        "    # print(data[\"text\"])\n",
        "    if data is not None and isinstance(data, dict) and \"text\" in data:\n",
        "        text = data[\"text\"]\n",
        "\n",
        "        # Check if text is not empty\n",
        "        if text:\n",
        "            # Chain regular expressions for better performance\n",
        "            cleaned_text = re.sub(chars_to_ignore_regex, \" \", text)\n",
        "            cleaned_text = re.sub(apostrophes_regex, \"'\", cleaned_text)\n",
        "            cleaned_text = re.sub(r\"'+\", \"'\", cleaned_text)\n",
        "            cleaned_text = re.sub(r\"([b-df-hj-np-tv-z])' ([aeiou])\", r\"\\1'\\2\", cleaned_text)\n",
        "            cleaned_text = re.sub(r\" '| '\", \" \", cleaned_text)\n",
        "            cleaned_text = re.sub(r\"' \", \" \", cleaned_text)\n",
        "            cleaned_text = re.sub(r\" +\", \" \", cleaned_text)\n",
        "\n",
        "            data[\"text\"] = cleaned_text\n",
        "        else:\n",
        "            print(f\"{data['text']} is empty\")\n",
        "    else:\n",
        "        print(\"data is None or not a dictionary\")\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "# a             ā, ã\n",
        "# e             ē\n",
        "# i             ī, i̇\n",
        "# o             ō\n",
        "# u             ū\n",
        "# c             č\n",
        "# //g             ğ\n",
        "# //k             ǩ\n",
        "# //s             š\n",
        "#// t             ŧ\n",
        "#// y             ȳ\n",
        "def replace_diacritics(data):\n",
        "    data[\"text\"] = re.sub(r\"[éèëēê]\", \"e\", data[\"text\"])\n",
        "    data[\"text\"] = re.sub(r\"[ãâāá]\", \"a\", data[\"text\"])\n",
        "    data[\"text\"] = re.sub(r\"[úūü]\", \"u\", data[\"text\"])\n",
        "    data[\"text\"] = re.sub(r\"[ôōó]\", \"o\", data[\"text\"])\n",
        "    data[\"text\"] = re.sub(r\"[ćç]\", \"c\", data[\"text\"])\n",
        "    data[\"text\"] = re.sub(r\"[ïī]\", \"i\", data[\"text\"])\n",
        "    data[\"text\"] = re.sub(r\"[ñ]\", \"n\", data[\"text\"])\n",
        "    data[\"text\"] = re.sub(r\"[ŧ]\", \"t\", data[\"text\"])\n",
        "    data[\"text\"] = re.sub(r\"[ȳ]\", \"y\", data[\"text\"])\n",
        "    data[\"text\"] = re.sub(r\"[š]\", \"s\", data[\"text\"])\n",
        "    data[\"text\"] = re.sub(r\"[ǩ]\", \"k\", data[\"text\"])\n",
        "    data[\"text\"] = re.sub(r\"[ğ]\", \"g\", data[\"text\"])\n",
        "    return data\n",
        "\n",
        "\n",
        "def remove_oov_characters(data):\n",
        "    oov_regex = \"[^ 'aiuenrbomkygwthszdcjfvplxq]\"\n",
        "    data[\"text\"] = re.sub(oov_regex, \"\", data[\"text\"])  # delete oov characters\n",
        "    data[\"text\"] = data[\"text\"].strip()\n",
        "    return data\n",
        "\n",
        "\n",
        "# Processing pipeline\n",
        "def apply_preprocessors(manifest, preprocessors):\n",
        "    for processor in preprocessors:\n",
        "        for idx in tqdm(range(len(manifest)), desc=f\"Applying {processor.__name__}\"):\n",
        "            manifest[idx] = processor(manifest[idx])\n",
        "\n",
        "    print(\"Finished processing manifest !\")\n",
        "    return manifest\n",
        "\n",
        "\n",
        "# List of pre-processing functions\n",
        "PREPROCESSORS = [\n",
        "    remove_special_characters,\n",
        "    replace_diacritics,\n",
        "    remove_oov_characters,\n",
        "]\n",
        "\n",
        "train_manifest = \"./drive/MyDrive/Mozilla/train_decoded.json\"\n",
        "eval_manifest = \"./drive/MyDrive/Mozilla/eval_decoded.json\"\n",
        "test_manifest = \"./drive/MyDrive/Mozilla/test_decoded.json\"\n",
        "\n",
        "train_data = read_manifest(train_manifest)\n",
        "eval_data = read_manifest(eval_manifest)\n",
        "test_data = read_manifest(test_manifest)\n",
        "\n",
        "\n",
        "\n",
        "# #Apply preprocessing\n",
        "train_data_processed = apply_preprocessors(train_data, PREPROCESSORS)\n",
        "eval_data_processed = apply_preprocessors(eval_data, PREPROCESSORS)\n",
        "test_data_processed = apply_preprocessors(test_data, PREPROCESSORS)\n",
        "\n",
        "## Write new manifests\n",
        "train_manifest_cleaned = write_processed_manifest(train_data_processed, train_manifest)\n",
        "eval_manifest_cleaned = write_processed_manifest(eval_data_processed, eval_manifest)\n",
        "test_manifest_cleaned = write_processed_manifest(test_data_processed, test_manifest)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a tokenized data to improve  quality and speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOb78By550sF"
      },
      "outputs": [],
      "source": [
        "# #Tokenizers\n",
        "!python  drive/MyDrive/Mozilla/scripts/process_asr_text_tokenizer.py \\\n",
        "  --manifest=drive/MyDrive/Mozilla/eval_decoded_processed.json,drive/MyDrive/Mozilla/train_decoded_processed.json \\\n",
        "  --vocab_size=1024 \\\n",
        "  --data_root=./drive/MyDrive/Mozilla/ \\\n",
        "  --tokenizer=\"spe\" \\\n",
        "  --spe_type=bpe \\\n",
        "  --spe_character_coverage=1.0 \\\n",
        "  --spe_max_sentencepiece_length=4 \\\n",
        "  --log\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To download respective  scripts may omit  they are available  in the  drive  link"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nEy2IMya2rw"
      },
      "outputs": [],
      "source": [
        "BRANCH = 'main'\n",
        "!wget -P drive/MyDrive/Mozilla/scripts/ https://raw.githubusercontent.com/NVIDIA/NeMo/main/scripts/speech_recognition/convert_to_tarred_audio_dataset.py\n",
        "!wget -P drive/MyDrive/Mozilla/scripts/ https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/asr/asr_ctc/speech_to_text_ctc_bpe.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This  cell will produce  one bucket of tarred  dataset  this will improve the speed of training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oY6pcxeR6WMD"
      },
      "outputs": [],
      "source": [
        "# # Tarred datasets and bucketing\n",
        "\n",
        "# create tarred dataset with 1 bucket\n",
        "!python drive/MyDrive/Mozilla/scripts/convert_to_tarred_audio_dataset.py \\\n",
        "  --manifest_path=drive/MyDrive/Mozilla/train_decoded_processed.json \\\n",
        "  --target_dir=drive/MyDrive/Mozilla/train_tarred_1bk \\\n",
        "  --num_shards=1024 \\\n",
        "  --max_duration=11.0 \\\n",
        "  --min_duration=1.0 \\\n",
        "  --shuffle \\\n",
        "  --shuffle_seed=1 \\\n",
        "  --sort_in_shards \\\n",
        "  --workers=-1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<b>Start Here   to train and next cells to transcribe</b>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below is the final  cell to run run to continue  training\n",
        "run  this  cell  to train the model\n",
        "After  it has  started to  execute  check  to  this  if the  will print\n",
        "Restoring all states  from SwahiliModelTraining1--val_wer=inf-epoch=9-last.ckpt   something like  this.\n",
        "It  will take approximate  of  2 and 30 minss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVmgayKchrJG"
      },
      "outputs": [],
      "source": [
        "# Training scripts and configs\n",
        "# /content/drive/MyDrive/Mozilla/train_tarred_1bk\n",
        "!python drive/MyDrive/Mozilla/scripts/speech_to_text_ctc_bpe.py \\\n",
        "--config-path=../conf/conformer/ \\\n",
        "--config-name=conformer_ctc_bpe \\\n",
        "exp_manager.name=\"SwahiliModelTraining1\" \\\n",
        "exp_manager.resume_if_exists=true \\\n",
        "exp_manager.resume_ignore_no_checkpoint=true \\\n",
        "exp_manager.exp_dir=drive/MyDrive/Mozilla/ \\\n",
        "model.tokenizer.dir=drive/MyDrive/Mozilla/tokenizer_spe_bpe_v1024_max_4 \\\n",
        "model.train_ds.is_tarred=true \\\n",
        "model.train_ds.tarred_audio_filepaths=drive/MyDrive/Mozilla/train_tarred_1bk/audio__OP_0..1023_CL_.tar \\\n",
        "model.train_ds.manifest_filepath=drive/MyDrive/Mozilla/train_tarred_1bk/tarred_audio_manifest.json \\\n",
        "model.validation_ds.manifest_filepath=/content/drive/MyDrive/Mozilla/eval_decoded_processed.json \\\n",
        "model.test_ds.manifest_filepath=drive/MyDrive/Mozilla/test_decoded_processed_v2.json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The  following  cell is to transcribe the test  data and produce  thejson file with description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTjrzq95ueRp"
      },
      "outputs": [],
      "source": [
        "# running interefence one\n",
        "# /content/drive/MyDrive/Mozilla/SwahiliModelTraining/checkpoints/SwahiliModelTraining.nemo\n",
        "!python drive/MyDrive/Mozilla/scripts/transcribe_speech.py \\\n",
        "  model_path=/content/drive/MyDrive/Mozilla/SwahiliModelTraining1/checkpoints/SwahiliModelTraining1.nemo \\\n",
        "  dataset_manifest=drive/MyDrive/Mozilla/test_decoded_processed_v2.json \\\n",
        "  output_filename=drive/MyDrive/Mozilla/results/test_with_predictions_4.json \\\n",
        "  batch_size=16 \\\n",
        "  cuda=0 \\\n",
        "  amp=True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below  Code is to transcribe  the eval audio  for submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwcFNRocun_J"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "!python drive/MyDrive/Mozilla/scripts/transcribe_speech.py \\\n",
        "  model_path=/content/drive/MyDrive/Mozilla/SwahiliModelTraining1/checkpoints/SwahiliModelTraining1.nemo \\\n",
        "  dataset_manifest=drive/MyDrive/Mozilla/Prime_eval.json \\\n",
        "  output_filename=drive/MyDrive/Mozilla/results/eval_version1.json \\\n",
        "  batch_size=16 \\\n",
        "  cuda=0 \\\n",
        "  amp=True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Following  Batch 1 Batch 2 and Batch 3  are chunk of eval data i chunked to decrease the load on the transcribe script  i then combine them with the code  in the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttP6qBFRQyYH"
      },
      "outputs": [],
      "source": [
        "# BATCH 1\n",
        "\n",
        "!python drive/MyDrive/Mozilla/scripts/transcribe_speech.py \\\n",
        "  model_path=/content/drive/MyDrive/Mozilla/SwahiliModelTraining1/checkpoints/SwahiliModelTraining1.nemo \\\n",
        "  dataset_manifest=drive/MyDrive/Mozilla/eval_decoded_processed_1.json \\\n",
        "  output_filename=drive/MyDrive/Mozilla/results/eval_version3_1.json \\\n",
        "  batch_size=16 \\\n",
        "  cuda=0 \\\n",
        "  amp=True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NW-EgC_NRSbY",
        "outputId": "4b5a054a-cf2a-44c1-f677-692b6f90bdc7"
      },
      "outputs": [],
      "source": [
        "# BATCH 2\n",
        "!python drive/MyDrive/Mozilla/scripts/transcribe_speech.py \\\n",
        "  model_path=/content/drive/MyDrive/Mozilla/SwahiliModelTraining1/checkpoints/SwahiliModelTraining1.nemo \\\n",
        "  dataset_manifest=drive/MyDrive/Mozilla/eval_decoded_processed_2.json \\\n",
        "  output_filename=drive/MyDrive/Mozilla/results/eval_version3_2.json \\\n",
        "  batch_size=16 \\\n",
        "  cuda=0 \\\n",
        "  amp=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GfdvwFXRPxy"
      },
      "outputs": [],
      "source": [
        "# BATCH 3\n",
        "# /content/drive/MyDrive/Mozillafggg\n",
        "!python /content/drive/MyDrive/Mozilla/scripts/transcribe_speech.py \\\n",
        "  model_path=/content/drive/MyDrive/Mozilla/SwahiliModelTraining1/checkpoints/SwahiliModelTraining1.nemo \\\n",
        "  dataset_manifest=/content/drive/MyDrive/Mozilla/eval_decoded_processed_3.json \\\n",
        "  output_filename=/content/drive/MyDrive/Mozilla/results/eval_version3_3.json \\\n",
        "  batch_size=16 \\\n",
        "  cuda=0 \\\n",
        "  amp=True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The  Code below  is to combine different json files to  one and  remove any duplicates maybe  found/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUKbfvYA-FwU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import jsonlines\n",
        "import csv\n",
        "\n",
        "###### List of input JSONL files to combine\n",
        "\n",
        "input_files = [\"eval_version3_1.json\",\"test_with_predictions_4.json\",]\n",
        "\n",
        "###### Output file where the combined JSONL will be stored\n",
        "\n",
        "file=r'drive/MyDrive/Mozilla/results/final_data.json'\n",
        "\n",
        "###### Function to combine multiple JSONL files\n",
        "\n",
        "def combine_jsonl(input_files, output_file):\n",
        "    with jsonlines.open(output_file, 'w') as writer:\n",
        "        for input_file in input_files:\n",
        "            file=f'drive/MyDrive/Mozilla/results/{input_file}'\n",
        "            with jsonlines.open(file) as reader:\n",
        "                for line in reader:\n",
        "                    writer.write(line)\n",
        "\n",
        "###### Combine the JSONL files\n",
        "\n",
        "combine_jsonl(input_files, file)\n",
        "\n",
        "##### Now to extract the paths\n",
        "JsonPath =r'drive/MyDrive/Mozilla/results/final_data.json'\n",
        "\n",
        "folder_path = r'drive/MyDrive/Mozilla/results/'\n",
        "\n",
        "csvName = \"eval_with_predictions.json\"\n",
        "data_list = {}  # List to store the JSON data\n",
        "\n",
        "sentence = []  # This list will store the processed data\n",
        "path = []  # This list will store the processed data\n",
        "\n",
        "with jsonlines.open(JsonPath) as reader:\n",
        "    for obj in reader:\n",
        "        file_path = os.path.basename(obj['audio_filepath'])\n",
        "        file_name = os.path.splitext[file_path](0) + \".mp3\"\n",
        "        path.append(file_name)  # the non-empty row to the output_data list\n",
        "        sentence.append(0 if not obj['pred_text'].strip() else obj['pred_text'])# the non-empty row to the output_data list\n",
        "SampeData = {\n",
        "    \"path\":path,\n",
        "    \"sentence\":sentence,\n",
        "\n",
        "}\n",
        "df = pd.DataFrame(SampeData)\n",
        "folder_path = r'drive/MyDrive/Mozilla/results/'\n",
        "df.to_csv(os.path.join(folder_path,'final_data.csv'),index=False)\n",
        "\n",
        "### This to remove duplicates incase there is \n",
        "def remove_duplicates(csv_path, column_name):\n",
        "    # Check if the file exists\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(f\"File not found: {csv_path}\")\n",
        "        return\n",
        "    # Read the CSV file into a DataFrame\n",
        "    df = pd.read_csv(csv_path)\n",
        "    # Remove duplicates based on the specified column\n",
        "    df_no_duplicates = df.drop_duplicates(subset=[column_name])\n",
        "    # Save the DataFrame with duplicates removed\n",
        "    df_no_duplicates.to_csv(csv_path, index=False)\n",
        "    print(f\"Duplicates removed from {csv_path}\")\n",
        "\n",
        "# Example usage\n",
        "csv_path = r'drive/MyDrive/Mozilla/results/final_data.csv'\n",
        "column_to_check = 'path'\n",
        "remove_duplicates(csv_path, column_to_check)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note  I found my prepared dataset didn't  have the following  files\n",
        "```common_voice_sw_28266617.mp3\n",
        "common_voice_sw_37214113.mp3\n",
        "common_voice_sw_37664539.mp3\n",
        "common_voice_sw_31290838.mp3\n",
        "common_voice_sw_35087387.mp3```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
